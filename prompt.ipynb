{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['conversation', 'word_count'], input_types={'conversation': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x708fc6b945e0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화하는 것입니다.'), additional_kwargs={}), MessagesPlaceholder(variable_name='conversation'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['word_count'], input_types={}, partial_variables={}, template='지금까지 대화를 {word_count} 단어로 요약합니다.'), additional_kwargs={})])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# message placeholder\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화하는 것입니다.',\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name='conversation'),\n",
    "        (\"human\", '지금까지 대화를 {word_count} 단어로 요약합니다.'),\n",
    "    ]\n",
    ")\n",
    "chat_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 당신은 요약 전문 AI 어시스턴트입니다. 당신의 임무는 주요 키워드로 대화하는 것입니다.\n",
      "Human: 안녕하세요! 저는 오늘 새로 입사한 황재성입니다. 만나서 반갑습니다.\n",
      "AI: 반가워요! 앞으로 잘 부탁드립니다.\n",
      "Human: 지금까지 대화를 5 단어로 요약합니다.\n"
     ]
    }
   ],
   "source": [
    "formatted_chat_prompt = chat_prompt.format(\n",
    "    word_count = 5,\n",
    "    conversation =[\n",
    "        (\"human\", '안녕하세요! 저는 오늘 새로 입사한 황재성입니다. 만나서 반갑습니다.'),\n",
    "        (\"ai\", '반가워요! 앞으로 잘 부탁드립니다.'),\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'새로 입사한 황재성 반가워요!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "\n",
    "chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"word_count\": 5,\n",
    "        \"conversation\": [\n",
    "            (\n",
    "                \"human\",\n",
    "                \"안녕하세요! 저는 오늘 새로 입사한 황재성 입니다. 만나서 반갑습니다.\",\n",
    "            ),\n",
    "            (\"ai\", \"반가워요! 앞으로 잘 부탁 드립니다.\"),\n",
    "        ],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n",
      "Answer:\n",
      "이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
      "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
      "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
      "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
      "최종 답변은: 아인슈타인\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fewshot prompt template\n",
    "\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "최종 답변은: 아인슈타인\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 네이버의 창립자는 누구인가요?\n",
    "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "추가 질문: 이해진은 언제 태어났나요?\n",
    "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "최종 답변은: 1967년 6월 22일\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "추가 질문: 신사임당은 언제 태어났나요?\n",
    "중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "최종 답변은: 연산군\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 올드보이의 감독은 누구인가요?\n",
    "중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "추가 질문: 기생충의 감독은 누구인가요?\n",
    "중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "최종 답변은: 예\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Question:\\n{question}\\nAnswer:\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: Google이 언제 창립되었나요?\n",
      "중간 답변: Google은 1998년에 창립되었습니다.\n",
      "추가 질문: Bill Gates는 언제 태어났나요?\n",
      "중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.\n",
      "최종 답변은: 1998년에 Bill Gates의 나이는 43살입니다."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_response\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI(model='gpt-4-turbo') | StrOutputParser()\n",
    "\n",
    "answer = chain.stream(\n",
    "    {'question': 'Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?'}\n",
    ")\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력에 가장 유사한 예시:\n",
      "Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\n",
      "\n",
      "question:\n",
      "네이버의 창립자는 언제 태어났나요?\n",
      "answer:\n",
      "이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 네이버의 창립자는 누구인가요?\n",
      "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
      "추가 질문: 이해진은 언제 태어났나요?\n",
      "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
      "최종 답변은: 1967년 6월 22일\n",
      "\n",
      "이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: Google이 창립된 연도는 언제인가요?\n",
      "중간 답변: Google은 1998년에 창립되었습니다.\n",
      "추가 질문: Bill Gates는 언제 태어났나요?\n",
      "중간 답변: Bill Gates는 1955년 10월 28일에 태어났습니다.\n",
      "최종 답변: Google이 창립된 1998년에 Bill Gates의 나이는 43살이었습니다."
     ]
    }
   ],
   "source": [
    "# example selector - to find the most similar example.\n",
    "\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector\n",
    ")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "chroma = Chroma('example_selector', OpenAIEmbeddings())\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    OpenAIEmbeddings(),\n",
    "    chroma,\n",
    "    k=1,\n",
    ")\n",
    "\n",
    "question = \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"\n",
    "\n",
    "selected_examples = example_selector.select_examples(\n",
    "    {\"question\": question}\n",
    ")\n",
    "\n",
    "print(f\"입력에 가장 유사한 예시:\\n{question}\\n\")\n",
    "for example in selected_examples:\n",
    "    print(f'question:\\n{example[\"question\"]}')\n",
    "    print(f'answer:\\n{example[\"answer\"]}')\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question:\\n{question}\\nAnswer:\",\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "chain = prompt | ChatOpenAI(model='gpt-4-turbo') \n",
    "\n",
    "answer = chain.stream(\n",
    "    {\"question\": \"Google이 창립된 연도에 Bill Gates의 나이는 몇 살인가요?\"}\n",
    ")\n",
    "\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/jaesung/anaconda3/envs/langchain/lib/python3.13/site-packages/langsmith/client.py:256: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# download prompt from langchain hub\n",
    "\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_kernel",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
